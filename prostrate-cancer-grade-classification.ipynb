{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18647,"databundleVersionId":1126921,"sourceType":"competition"},{"sourceId":1101206,"sourceType":"datasetVersion","datasetId":615046}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hrithikmajumdar/prostrate-cancer-grade-classification?scriptVersionId=187008658\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"hello\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/prostate-cancer-grade-assessment/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install openslide-python","metadata":{"execution":{"iopub.status.busy":"2024-07-04T16:04:12.573412Z","iopub.execute_input":"2024-07-04T16:04:12.574318Z","iopub.status.idle":"2024-07-04T16:04:24.69093Z","shell.execute_reply.started":"2024-07-04T16:04:12.574282Z","shell.execute_reply":"2024-07-04T16:04:24.689821Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openslide-python in /opt/conda/lib/python3.10/site-packages (1.3.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from openslide-python) (9.5.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**NORMAL Boudning Box way**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport base64\nimport openslide\n\n# Define paths to the folders\ntrain_images_path = '/kaggle/input/prostate-cancer-grade-assessment/train_images'\ntrain_label_masks_path = '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks'\n\n# Function to encode the image to base64\ndef encode_image_base64(image):\n    _, buffer = cv2.imencode('.jpg', image)\n    image_base64 = base64.b64encode(buffer).decode('utf-8')\n    return image_base64\n\n# Function to read images and masks using OpenSlide\ndef read_image_mask(image_id):\n    image_path = os.path.join(train_images_path, f\"{image_id}.tiff\")\n    mask_path = os.path.join(train_label_masks_path, f\"{image_id}.tiff\")\n    \n    # Debugging: Print file paths\n    print(f\"Reading image: {image_path}\")\n    print(f\"Reading mask: {mask_path}\")\n    \n    if not os.path.exists(image_path):\n        print(f\"Image not found for ID: {image_id}\")\n        return None, None\n    if not os.path.exists(mask_path):\n        print(f\"Mask not found for ID: {image_id}\")\n        return None, None\n    \n    image_slide = openslide.OpenSlide(image_path)\n    mask_slide = openslide.OpenSlide(mask_path)\n    \n    # Read the images at level 0 (full resolution)\n    image = np.array(image_slide.read_region((0, 0), 0, image_slide.level_dimensions[0]))\n    mask = np.array(mask_slide.read_region((0, 0), 0, mask_slide.level_dimensions[0]))\n    \n    # Convert mask image to grayscale\n    mask = cv2.cvtColor(mask, cv2.COLOR_RGBA2GRAY)\n    \n    return image, mask\n\n# Function to process each image and mask\ndef process_image_mask(image_id):\n    # Read the image and mask using OpenSlide\n    image, mask = read_image_mask(image_id)\n    \n    if image is None or mask is None:\n        return (image_id, None, None, None, None, None)\n    \n    # Debugging: Check if the images are read correctly\n    print(f\"Image shape for ID {image_id}: {image.shape}\")\n    print(f\"Mask shape for ID {image_id}: {mask.shape}\")\n    \n    # Find contours in the mask image\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Debugging: Check if contours are found\n    print(f\"Number of contours found for ID {image_id}: {len(contours)}\")\n    \n    # Assuming the largest contour is the one we want\n    if contours:\n        largest_contour = max(contours, key=cv2.contourArea)\n        \n        # Get the bounding box coordinates of the largest contour\n        x, y, w, h = cv2.boundingRect(largest_contour)\n        \n        # Crop the corresponding portion from the main image\n        roi = image[y:y+h, x:x+w]\n        \n        # Debugging: Check the ROI shape\n        print(f\"ROI shape for ID {image_id}: {roi.shape}\")\n        \n        # Encode the ROI to base64\n        roi_base64 = encode_image_base64(roi)\n        \n        return (image_id, x, y, w, h, roi_base64)\n    else:\n        print(f\"No contours found for ID {image_id}\")\n        return (image_id, None, None, None, None, None)\n\n# List all files in the train_label_masks directory\nmask_files = set(os.path.splitext(filename.lower())[0] for filename in os.listdir(train_label_masks_path))\nimage_files = set(os.path.splitext(filename.lower())[0] for filename in os.listdir(train_images_path))\n\n# Get the list of common image IDs (assuming the filenames without extension are the image IDs)\nimage_ids = image_files.intersection(mask_files)\n\nprint(f\"Total images found: {len(image_files)}\")\nprint(f\"Total masks found: {len(mask_files)}\")\nprint(f\"Total common IDs: {len(image_ids)}\")\n\n# Process each image and mask and collect the results\nresults = []\nfor image_id in image_ids:\n    results.append(process_image_mask(image_id))\n\n# Create a DataFrame to hold the results\nroi_df = pd.DataFrame(results, columns=['image_id', 'x', 'y', 'width', 'height', 'roi_base64'])\n\n# Save the DataFrame to a CSV file\nroi_df.to_csv('rois_with_coordinates.csv', index=False)\n\n# Debugging: Print the resulting DataFrame\nprint(roi_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:25:22.768018Z","iopub.execute_input":"2024-07-04T17:25:22.768867Z","iopub.status.idle":"2024-07-04T17:25:22.852482Z","shell.execute_reply.started":"2024-07-04T17:25:22.768836Z","shell.execute_reply":"2024-07-04T17:25:22.851622Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Total images found: 10616\nTotal masks found: 10516\nTotal common IDs: 0\nEmpty DataFrame\nColumns: [image_id, x, y, width, height, roi_base64]\nIndex: []\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**BITWISE AND**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport base64\nimport openslide\n\n# Define paths to the folders\ntrain_images_path = '/kaggle/input/prostate-cancer-grade-assessment/train_images'\ntrain_label_masks_path = '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks'\n\n# Function to encode the image to base64\ndef encode_image_base64(image):\n    _, buffer = cv2.imencode('.jpg', image)\n    image_base64 = base64.b64encode(buffer).decode('utf-8')\n    return image_base64\n\n# Function to read images and masks using OpenSlide\ndef read_image_mask(image_id):\n    image_path = os.path.join(train_images_path, f\"{image_id}.tiff\")\n    mask_path = os.path.join(train_label_masks_path, f\"{image_id}.tiff\")\n    \n    if not os.path.exists(image_path):\n        print(f\"Image not found for ID: {image_id}\")\n        return None, None\n    if not os.path.exists(mask_path):\n        print(f\"Mask not found for ID: {image_id}\")\n        return None, None\n    \n    image_slide = openslide.OpenSlide(image_path)\n    mask_slide = openslide.OpenSlide(mask_path)\n    \n    # Read the images at level 0 (full resolution)\n    image = np.array(image_slide.read_region((0, 0), 0, image_slide.level_dimensions[0]))\n    mask = np.array(mask_slide.read_region((0, 0), 0, mask_slide.level_dimensions[0]))\n    \n    # Convert mask image to grayscale\n    mask = cv2.cvtColor(mask, cv2.COLOR_RGBA2GRAY)\n    \n    return image, mask\n\n# Function to process each image and mask\ndef process_image_mask(image_id):\n    # Read the image and mask using OpenSlide\n    image, mask = read_image_mask(image_id)\n    \n    if image is None or mask is None:\n        return (image_id, None, None, None, None, None)\n    \n    # Ensure mask is binary\n    _, mask_binary = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n    \n    # Perform bitwise AND operation\n    roi = cv2.bitwise_and(image, image, mask=mask_binary)\n    \n    # Find contours in the mask image\n    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    if contours:\n        largest_contour = max(contours, key=cv2.contourArea)\n        \n        # Get the bounding box coordinates of the largest contour\n        x, y, w, h = cv2.boundingRect(largest_contour)\n        \n        # Crop the corresponding portion from the ROI\n        cropped_roi = roi[y:y+h, x:x+w]\n        \n        # Encode the ROI to base64\n        roi_base64 = encode_image_base64(cropped_roi)\n        \n        return (image_id, x, y, w, h, roi_base64)\n    else:\n        return (image_id, None, None, None, None, None)\n\n# List all files in the train_label_masks directory\nmask_files = set(os.path.splitext(filename.lower())[0] for filename in os.listdir(train_label_masks_path))\nimage_files = set(os.path.splitext(filename.lower())[0] for filename in os.listdir(train_images_path))\n\n# Get the list of common image IDs (assuming the filenames without extension are the image IDs)\nimage_ids = image_files.intersection(mask_files)\n\nprint(f\"Total images found: {len(image_files)}\")\nprint(f\"Total masks found: {len(mask_files)}\")\nprint(f\"Total common IDs: {len(image_ids)}\")\n\n# Process each image and mask and collect the results\nresults = []\nfor image_id in image_ids:\n    results.append(process_image_mask(image_id))\n\n# Create a DataFrame to hold the results\nroi_df = pd.DataFrame(results, columns=['image_id', 'x', 'y', 'width', 'height', 'roi_base64'])\n\n# Save the DataFrame to a CSV file\nroi_df.to_csv('rois_with_coordinates.csv', index=False)\n\n# Debugging: Print the resulting DataFrame\nprint(roi_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:09:40.394676Z","iopub.execute_input":"2024-07-04T17:09:40.395442Z","iopub.status.idle":"2024-07-04T17:09:40.476264Z","shell.execute_reply.started":"2024-07-04T17:09:40.395404Z","shell.execute_reply":"2024-07-04T17:09:40.475392Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Total images found: 10616\nTotal masks found: 10516\nTotal common IDs: 0\nEmpty DataFrame\nColumns: [image_id, x, y, width, height, roi_base64]\nIndex: []\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install tifffile","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:00:01.181261Z","iopub.execute_input":"2024-07-04T17:00:01.181625Z","iopub.status.idle":"2024-07-04T17:00:13.248255Z","shell.execute_reply.started":"2024-07-04T17:00:01.181594Z","shell.execute_reply":"2024-07-04T17:00:13.247104Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tifffile in /opt/conda/lib/python3.10/site-packages (2023.12.9)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tifffile) (1.26.4)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport pandas as pd\nfrom tifffile import imread\n\n# Define paths to folders\ntrain_images_path = \"/kaggle/input/prostate-cancer-grade-assessment/train_images\"\ntrain_label_masks_path = \"/kaggle/input/prostate-cancer-grade-assessment/train_label_masks\"\n\n# Define empty list to store ROI data (x1, y1, x2, y2)\nroi_data = []\n\n# Loop through each mask image\nfor mask_filename in os.listdir(train_label_masks_path):\n  #mask = imread(os.path.join(train_label_masks_path, mask_filename))\n  # Load the mask image (assuming grayscale)\n  mask = cv2.imread(os.path.join(train_label_masks_path, mask_filename), cv2.IMREAD_GRAYSCALE)\n\n  # Find contours in the mask\n  contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n  # Assuming one ROI per mask (modify if needed)\n  if len(contours) > 0:\n    # Get bounding box of the first contour\n    x, y, w, h = cv2.boundingRect(contours[0])\n\n    # Extract corresponding image filename (assuming same names)\n    image_filename = mask_filename.replace(\"_mask.png\", \".png\")  # Modify extension if needed\n\n    # Load the corresponding image from train_images\n    image = cv2.imread(os.path.join(train_images_path, image_filename))\n\n    # Calculate ROI coordinates for the main image\n    roi_x1, roi_y1 = x, y\n    roi_x2, roi_y2 = x + w, y + h\n\n    # Append ROI data to the list\n    roi_data.append([roi_x1, roi_y1, roi_x2, roi_y2, image_filename])\n\n# Create pandas dataframe from the list\ndf = pd.DataFrame(roi_data, columns=[\"x1\", \"y1\", \"x2\", \"y2\", \"image_filename\"])\n\n# Save the dataframe as a CSV file\ndf.to_csv(\"roi_data.csv\", index=False)\n\nprint(\"ROI data saved to roi_data.csv!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:06:06.943636Z","iopub.execute_input":"2024-07-04T17:06:06.944544Z","iopub.status.idle":"2024-07-04T17:06:38.896812Z","shell.execute_reply.started":"2024-07-04T17:06:06.944507Z","shell.execute_reply":"2024-07-04T17:06:38.895756Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"[ WARN:0@3698.016] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/c699f572c40dbee492ca9ac0c1a0d9f7_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3698.527] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/78138d4fc0b8fceb04138cd44d4a4fdd_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3699.195] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/8bf80024b1a7f12ce52136af81a930df_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3703.924] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/b4f3528c1ab8e7ae2faac094cdbe06c3_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3705.084] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/1abb6882a5cba3d9c166736825bbe101_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3707.266] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/67221cc31ec6ad9e106de548f7aa5823_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3708.811] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/f5d72b1f141a139c46446d263ceb5e7c_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3717.822] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/f526a2ca5612d5913984a67edabaf991_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3718.898] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/e173f2b10faccbb5c9f5a0c38a2a1fd0_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3722.403] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/f750891191013386453c29c16670e951_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3723.190] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/1d33889037497d32695ac48a786af6d5_mask.tiff'): can't open/read file: check file path/integrity\n[ WARN:0@3724.679] global loadsave.cpp:248 findDecoder imread_('/kaggle/input/prostate-cancer-grade-assessment/train_images/0a5d6a20d1429dd55b94ae1b857ad573_mask.tiff'): can't open/read file: check file path/integrity\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Loop through each mask image\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mask_filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(train_label_masks_path):\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;66;03m#mask = imread(os.path.join(train_label_masks_path, mask_filename))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;66;03m# Load the mask image (assuming grayscale)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m   mask \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_label_masks_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_filename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;66;03m# Find contours in the mask\u001b[39;00m\n\u001b[1;32m     19\u001b[0m   contours, _ \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindContours(mask, cv2\u001b[38;5;241m.\u001b[39mRETR_EXTERNAL, cv2\u001b[38;5;241m.\u001b[39mCHAIN_APPROX_SIMPLE)\n","\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:79: error: (-215:Assertion failed) pixels <= CV_IO_MAX_IMAGE_PIXELS in function 'validateInputImageSize'\n"],"ename":"error","evalue":"OpenCV(4.9.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:79: error: (-215:Assertion failed) pixels <= CV_IO_MAX_IMAGE_PIXELS in function 'validateInputImageSize'\n","output_type":"error"}]},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/working/rois_with_coordinates.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-07-04T16:41:43.625723Z","iopub.execute_input":"2024-07-04T16:41:43.626363Z","iopub.status.idle":"2024-07-04T16:41:43.64376Z","shell.execute_reply.started":"2024-07-04T16:41:43.626326Z","shell.execute_reply":"2024-07-04T16:41:43.643068Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [image_id, x, y, width, height, roi_base64]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>x</th>\n      <th>y</th>\n      <th>width</th>\n      <th>height</th>\n      <th>roi_base64</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_gleason_score(score):\n    try:\n        parts = score.split('+')\n        return int(parts[0]) + int(parts[1])\n    except (ValueError, AttributeError):\n        return np.nan\n\ntrain_df['gleason_score_numeric'] = train_df['gleason_score'].apply(convert_gleason_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x='isup_grade', data=train_df)\nplt.title('Distribution of ISUP Grade')\n\nfor p in plt.gca().patches:\n    plt.gca().annotate(f\"{p.get_height()}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='gleason_score_numeric', data=train_df)\nplt.title('Distribution of Gleason Score (Numeric)')\n\nfor p in plt.gca().patches:\n    plt.gca().annotate(f\"{p.get_height()}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_images/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_ids = train_df['image_id'].iloc[:9].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import openslide","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = train_df['image_id'].iloc[0]\nfull_image_path = image_dir + image_id + '.tiff'\n\ntry:\n\n    example = openslide.OpenSlide(full_image_path)\n\n    clipped_example = example.read_region((5000, 5000), 0, (image_size, image_size))\n\n    plt.imshow(clipped_example)\n    plt.title(f\"Image ID: {image_id}\")\n    plt.axis('off')\n\n    example.close()\nexcept Exception as e:\n    print(f\"Error loading image {image_id}: {e}\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"image_path\"] = [image_dir+image_id+\".tiff\" for image_id in train_df[\"image_id\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=train_df, x='isup_grade')\nplt.title('Distribution of ISUP grades')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.pie(train_df['isup_grade'].value_counts(), labels=train_df['isup_grade'].unique(), autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of ISUP grades')\nplt.axis('equal')  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=train_df, x='data_provider')\nplt.title('Distribution of Data Providers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.pie(train_df['data_provider'].value_counts(), labels=train_df['data_provider'].unique(), autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of Data Providers')\nplt.axis('equal')  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=train_df, x='gleason_score_numeric')\nplt.title('Distribution of Gleason Scores')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = train_df[['isup_grade', 'gleason_score_numeric']].corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_item_count = int(len(train_df) * 0.8)\nvalidation_df = train_df[training_item_count:]\ntrain_df = train_df[:training_item_count]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_single_sample(image_path, image_size=256, training=False, display=False):\n    image = openslide.OpenSlide(image_path)\n    mask_path = image_path.replace(\"train_images\", \"train_label_masks\").replace(\".tiff\", \"_mask.tiff\")\n    mask = openslide.OpenSlide(mask_path)\n    \n    stacked_image = []\n    groundtruth_per_image = []\n    \n    maximum_iteration = 0\n    selected_sample = False\n    while not selected_sample:\n        sampling_start_x = randint(image_size, image.dimensions[0] - image_size)\n        sampling_start_y = randint(image_size, image.dimensions[1] - image_size)\n\n        clipped_sample = image.read_region((sampling_start_x, sampling_start_y), 0, (256, 256))\n        clipped_array = np.asarray(clipped_sample)\n        \n        if (not np.all(clipped_array == 255) and np.std(clipped_array) > 20) or maximum_iteration > 200:\n            if display:\n                plt.imshow(clipped_sample)\n                plt.show()\n                \n            sampled_image = clipped_array[:, :, :3]\n            \n            if training:\n                clipped_mask = mask.read_region((sampling_start_x, sampling_start_y), 0, (256, 256))\n                groundtruth_per_image.append(np.mean(np.asarray(clipped_mask)[:, :, 0]))\n            \n            selected_sample = True\n        maximum_iteration += 1\n    \n    if training: \n        return np.array(sampled_image), np.array(groundtruth_per_image)\n    else:\n        return np.array(sampled_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_random_samples(image_path, image_size=256, display=False):\n    image = openslide.OpenSlide(image_path)\n    stacked_image = []\n    \n    selected_samples = 0\n    maximum_iteration = 0\n    while selected_samples < 3:\n        sampling_start_x = randint(image_size, image.dimensions[0] - image_size)\n        sampling_start_y = randint(image_size, image.dimensions[1] - image_size)\n\n        clipped_sample = image.read_region((sampling_start_x, sampling_start_y), 0, (256, 256))\n        clipped_array = np.asarray(clipped_sample)\n        \n        if (not np.all(clipped_array == 255) and np.std(clipped_array) > 20) or maximum_iteration > 200:\n            if display:\n                plt.imshow(clipped_sample)\n                plt.show()\n\n            stacked_image.append(clipped_array[:, :, :3])\n            selected_samples += 1\n        maximum_iteration += 1\n    return np.array(stacked_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_single_image_generator(image_path_list, batch_size=16):\n    while True:\n        for start in range(0, len(image_path_list), batch_size):\n            X_batch = []\n            Y_batch = []\n            end = min(start + batch_size, training_item_count)\n\n            image_info_list = [get_single_sample(image_path, training=True) for image_path in image_path_list[start:end]]\n            X_batch = np.array([image_info[0]/255. for image_info in image_info_list])\n            Y_batch = np.array([image_info[1] for image_info in image_info_list])\n            \n            yield X_batch, Y_batch ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import randint ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_samples = get_random_samples(train_df.iloc[0].image_path, display=True)\nprint(\"Random samples shape:\", random_samples.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = train_df.iloc[0].image_path  \nsample_image, groundtruth = get_single_sample(image_path, training=True, display=True)\n\nprint(\"Sample Image Shape:\", sample_image.shape)\nprint(\"Ground Truth:\", groundtruth)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport PIL\nfrom IPython.display import Image, display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom keras.models import Sequential, Model,load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten,BatchNormalization,Activation\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import to_categorical\nimport gc\nimport skimage.io\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow.python.keras import backend as K\nsess = K.get_session()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=openslide.OpenSlide('/kaggle/input/prostate-cancer-grade-assessment/train_images/2fd1c7dc4a0f3a546a59717d8e9d28c3.tiff')\ndisplay(img.get_thumbnail(size=(512,512)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img.dimensions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['isup_grade'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=[]\ndata=[]\ndata_dir='/kaggle/input/panda-resized-train-data-512x512/train_images/train_images/'\nfor i in range(train_df.shape[0]):\n    data.append(data_dir + train_df['image_id'].iloc[i]+'.png')\n    labels.append(train_df['isup_grade'].iloc[i])\ndf=pd.DataFrame(data)\ndf.columns=['images']\ndf['isup_grade']=labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_images(image_paths, num_images=5):\n    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n    for i in range(num_images):\n        image_path = image_paths[i]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n        axes[i].imshow(image)\n        axes[i].axis('off')\n        axes[i].set_title(f'Image {i+1}')\n    plt.show()\n\nimage_paths = df['images'].tolist()\n\ndisplay_images(image_paths, num_images=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('/kaggle/working/prostrate.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_size = 900\n\ngrouped = df.groupby('isup_grade')\n\ndf_new = pd.DataFrame(columns=df.columns)\n\nfor isup_grade, group in grouped:\n    if len(group) >= sample_size:\n        sampled_group = group.sample(n=sample_size, random_state=42)\n    else:\n        sampled_group = group.sample(n=len(group), random_state=42)\n    df_new = pd.concat([df_new, sampled_group])\n\ndf_new.reset_index(drop=True, inplace=True)\n\nprint(\"Shape of df_new:\", df_new.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_val_df, test_df = train_test_split(df_new, test_size=0.15, random_state=42, stratify=df_new['isup_grade'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_val_df, test_size=0.1765, random_state=42, stratify=train_val_df['isup_grade'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total samples: {len(df_new)}\")\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"Test samples: {len(test_df)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport shutil\nimport pathlib\nimport itertools\nfrom PIL import Image\n\nimport cv2\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\nfrom tensorflow.keras import regularizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['isup_grade'] = train_df['isup_grade'].astype(str)\nval_df['isup_grade'] = val_df['isup_grade'].astype(str)\ntest_df['isup_grade'] = test_df['isup_grade'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nimg_size = (224, 224)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\n\ntr_gen = ImageDataGenerator(rescale=1./255,rotation_range=20, width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\nts_gen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = tr_gen.flow_from_dataframe(train_df, x_col= 'images', y_col= 'isup_grade', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\nvalid_gen = ts_gen.flow_from_dataframe(val_df, x_col= 'images', y_col= 'isup_grade', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\ntest_gen = ts_gen.flow_from_dataframe(test_df, x_col= 'images', y_col= 'isup_grade', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= False, batch_size= batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"policy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import InceptionV3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D\n\ndef inceptionv3_model(num_classes=None):\n    model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    x = GlobalAveragePooling2D()(model.output)  \n    output = Dense(num_classes, activation='softmax')(x)\n    model = Model(model.input, output)\n    return model\n\ninceptionv3_conv = inceptionv3_model(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KappaScoreCallback(Callback):\n    def __init__(self, validation_data):\n        super().__init__()\n        self.validation_data = validation_data\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_gen = self.validation_data\n        val_true = []\n        val_pred = []\n        for i in range(len(val_gen)):\n            x_val, y_val = val_gen[i]\n            val_true.extend(tf.argmax(y_val, axis=1).numpy())\n            val_pred.extend(tf.argmax(self.model.predict(x_val), axis=1).numpy())\n\n        kappa = cohen_kappa_score(val_true, val_pred)\n        print(f\"\\nEpoch {epoch + 1} - Cohen's Kappa: {kappa:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nopt = Adam(learning_rate=0.001)\ninceptionv3_conv.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"opt = SGD(learning_rate=0.001)\ninceptionv3_conv.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:46:10.829609Z","iopub.execute_input":"2024-06-05T06:46:10.829865Z","iopub.status.idle":"2024-06-05T06:46:10.855661Z","shell.execute_reply.started":"2024-06-05T06:46:10.829843Z","shell.execute_reply":"2024-06-05T06:46:10.854956Z"}}},{"cell_type":"code","source":"nb_epochs = 75\nbatch_size=16\nnb_train_steps = train_df.shape[0]//batch_size\nnb_val_steps=val_df.shape[0]//batch_size\nprint(\"Number of training and validation steps: {} and {}\".format(nb_train_steps,nb_val_steps))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = inceptionv3_conv.fit(\n    train_gen,\n    steps_per_epoch=nb_train_steps,\n    epochs=nb_epochs,\n    validation_data=valid_gen,\n    validation_steps=nb_val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='upper left')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_gen.reset()\nY_pred = inceptionv3_conv.predict(test_gen, steps=len(test_gen), verbose=1)\ny_pred = np.argmax(Y_pred, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = test_gen.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_labels = list(test_gen.class_indices.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_report = classification_report(y_true, y_pred, target_names=class_labels)\nprint('Classification Report')\nprint(class_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kappa_callback = KappaScoreCallback(validation_data=valid_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vgg16_model(num_classes=None):\n    model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    x = Flatten()(model.output)\n    output = Dense(num_classes, activation='softmax')(x)\n    model = Model(model.input, output)\n    return model\n\nvgg_conv = vgg16_model(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import cohen_kappa_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.callbacks import Callback","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"class KappaScoreCallback(Callback):\n    def __init__(self, validation_data):\n        super().__init__()\n        self.validation_data = validation_data\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_gen = self.validation_data\n        val_true = []\n        val_pred = []\n        for i in range(len(val_gen)):\n            x_val, y_val = val_gen[i]\n            val_true.extend(tf.argmax(y_val, axis=1).numpy())\n            val_pred.extend(tf.argmax(self.model.predict(x_val), axis=1).numpy())\n\n        kappa = cohen_kappa_score(val_true, val_pred)\n        print(f\"\\nEpoch {epoch + 1} - Cohen's Kappa: {kappa:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:52:14.671902Z","iopub.execute_input":"2024-06-04T09:52:14.672891Z","iopub.status.idle":"2024-06-04T09:52:14.6803Z","shell.execute_reply.started":"2024-06-04T09:52:14.672857Z","shell.execute_reply":"2024-06-04T09:52:14.679321Z"}}},{"cell_type":"code","source":"opt = Adam(learning_rate=0.001)\n#opt = SGD(learning_rate=0.001)\nvgg_conv.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"nb_epochs = 50\nbatch_size=16\nnb_train_steps = train_df.shape[0]//batch_size\nnb_val_steps=val_df.shape[0]//batch_size\nprint(\"Number of training and validation steps: {} and {}\".format(nb_train_steps,nb_val_steps))","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:52:28.526342Z","iopub.execute_input":"2024-06-04T09:52:28.526712Z","iopub.status.idle":"2024-06-04T09:52:28.532443Z","shell.execute_reply.started":"2024-06-04T09:52:28.526685Z","shell.execute_reply":"2024-06-04T09:52:28.53159Z"}}},{"cell_type":"code","source":"# kappa_callback = KappaScoreCallback(validation_data=valid_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = vgg_conv.fit(\n    train_gen,\n    steps_per_epoch=nb_train_steps,\n    epochs=nb_epochs,\n    validation_data=valid_gen,\n    validation_steps=nb_val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"history = inceptionv3_conv.fit(\n    train_gen,\n    steps_per_epoch=nb_train_steps,\n    epochs=nb_epochs,\n    validation_data=valid_gen,\n    validation_steps=nb_val_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:53:28.776832Z","iopub.execute_input":"2024-06-04T09:53:28.77768Z","iopub.status.idle":"2024-06-04T10:47:52.20591Z","shell.execute_reply.started":"2024-06-04T09:53:28.777647Z","shell.execute_reply":"2024-06-04T10:47:52.205055Z"}}},{"cell_type":"code","source":"plt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='upper left')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test_gen.reset()\nY_pred = inceptionv3_conv.predict(test_gen, steps=len(test_gen), verbose=1)\ny_pred = np.argmax(Y_pred, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:55:45.78656Z","iopub.execute_input":"2024-06-04T10:55:45.786875Z","iopub.status.idle":"2024-06-04T10:55:51.000409Z","shell.execute_reply.started":"2024-06-04T10:55:45.78685Z","shell.execute_reply":"2024-06-04T10:55:50.99953Z"}}},{"cell_type":"code","source":"test_gen.reset()\nY_pred = vgg_conv.predict(test_gen, steps=len(test_gen), verbose=1)\ny_pred = np.argmax(Y_pred, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_true = test_gen.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_labels = list(test_gen.class_indices.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_report = classification_report(y_true, y_pred, target_names=class_labels)\nprint('Classification Report')\nprint(class_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNet\n\ndef mobilenet_model(num_classes=None):\n    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    x = GlobalAveragePooling2D()(base_model.output)\n    output = Dense(num_classes, activation='softmax')(x)\n    model = Model(base_model.input, output)\n    return model\n\n = mobilenet_model(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = Adam(learning_rate=0.001)\nmobilenet_model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_gen.reset()\nY_pred = mobilenet_model.predict(test_gen, steps=len(test_gen), verbose=1)\ny_pred = np.argmax(Y_pred, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='upper left')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = mobilenet_model.fit(\n    train_gen,\n    steps_per_epoch=nb_train_steps,\n    epochs=nb_epochs,\n    validation_data=valid_gen,\n    validation_steps=nb_val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_report = classification_report(y_true, y_pred, target_names=class_labels)\nprint('Classification Report')\nprint(class_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}